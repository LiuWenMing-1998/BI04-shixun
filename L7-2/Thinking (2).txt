Thinking2参数共享指的是什么？
一个卷积层中可以有多个不同的卷积核，而每一个卷积核都对应着一个滤波后映射出的新图像(Feature map),同一个新图像的所有像素全部来自于同一个卷积核，这就是卷积核的参数共享。

Thinking3为什么会用到batch normalization ?
BN就是通过一定的规范化手段，把每层神经网络任意神经元　这个输入值的分布强行拉回到均值为0方差为1的标准正态分布；它使得非线性变换函数的输入值落入对输入比较敏感的区域，从而避免梯度消失问题。这样输入的小变化就会导致损失函数较大的变化（使得梯度变大，避免梯度消失问题产生），同时也让收敛速度更快，加快训练速度。

Thinking4使用dropout可以解决什么问题？
可以减少过拟合的问题，再工程上对数据进行增强



